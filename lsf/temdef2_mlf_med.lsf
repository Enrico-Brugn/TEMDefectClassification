#!/bin/bash

#BSUB -J "ae_mlflow_med"
#BSUB -q prod.med
#BSUB -n 1
#BSUB -R "rusage[ngpus_excl_p=1] span[hosts=1]"
#BSUB -e "../results/derr/%J.stderr"
#BSUB -o "../results/dout/%J.stdout"


module purge 
module load Anaconda3/2019.03-rhel-7


cd ../scripts


export MLFLOW_TRACKING_URI=http://mlflow2.temdefvm1.zc2.ibm.com:5000
export MLFLOW_S3_ENDPOINT_URL=http://mlflow2.temdefvm1.zc2.ibm.com:9000
export AWS_ACCESS_KEY_ID=access-key
export AWS_SECRET_ACCESS_KEY=secret-key
export MLFLOW_EXPERIMENT_NAME=enn_gpu_autoencoder

#mlflow experiments create --artifact-location s3://getting-started-mlflow -n ${MLFLOW_EXPERIMENT_NAME}


#Parsing Parameters
#AUGMENTATION=$"0"
#TEST=$"False"
#MODEL=$"0"
#TOPOLOGY=$"contracting"
#LOSS=$"mse_spectral"
#EPOCHS=$"5"
#STEPS=$"8"
#BATCH_SIZE=$"25"
#LEARNING_RATE=$"0.00001"
#BETA=$"100"
#KSIZE=$"10"

python3 run.py -name "vae_contracting_msespec" -t "contracting" -d 3 -f 128 -e 200 -s 32 -b 32 


: << ////
python3 run.py -name "vae_mse" -d 1 -var True -l 'mse'# -e 80 -s 64 -b 32

: << ////

python3 run.py -name $ID -t 'contracting' -d 5 -l 'mse_spectral' -e 120 -s 32 -b 32  -beta 100 -k 16 -fold 0

#mlflow run . --no-conda -P batchsize=20 #augmentation=$AUGMENTATION  -P test=$TEST  -P model=$MODEL  -P topology=$TOPOLOGY  -P loss=$LOSS  -P epochs=$EPOCHS  -P steps=$STEPS  -P batch_size=$BATCH_SIZE  -P learning_rate=$LEARNING_RATE  -P beta=$BETA  -P ksize=$KSIZE

: << ////
python3 run.py -l 'mse_spectral' -e 80 -s 32 -b 16 -lr 0.0001
python3 run.py -l 'mse_spectral' -e 150 -s 32 -b 16 -lr 0.0001
python3 run.py -l 'mse_spectral' -e 300 -s 32 -b 16 -lr 0.0001
python3 run.py -l 'mse_spectral' -e 500 -s 32 -b 16 -lr 0.0001
////
